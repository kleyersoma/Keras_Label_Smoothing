{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body style=\"font-family: Arial;font-size: 17px;\">\n",
    "    <div style=\"position: relative; max-width: 100%;margin: auto; vertical-align: middle;\">\n",
    "      <img src=\"images/0.jpg\" alt=\"Notebook\" style=\"width:100%;\">\n",
    "      <div style=\"position: absolute; top: 0; padding: 25px; font-size:28px; color: white;\">\n",
    "          <h1>Label <i>Smoothing</i></h1>\n",
    "        </div>\n",
    "      <div style=\"position: absolute;bottom: 0;background: rgb(0, 0, 0);background: rgba(0, 0, 0, 0.5);color: #f1f1f1;width: 100%;padding: 20px;\">\n",
    "        <h1>What is label smoothing and why would it be used?</h1>\n",
    "        <p>The model should not become too confident in its predictions, this can be avoided by applying label smoothing, this technique can lessen the confidence of the model and prevent it from descending into deep crevices of the loss landscape where overfitting occurs. <i>Label Smoothing is form of regularization.</i></p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <p>\n",
    "        There a two methods to implement <b>Label <i>Smoothing</i></b>:\n",
    "        <ul>\n",
    "            <li>Label smoothing by <b>explicitly updating your labels list</b>.</li>\n",
    "            <li>Label smoothing by <b>using the loss function</b>.</li>\n",
    "        </ul>\n",
    "    <br/>\n",
    "    Regularization methods are used to help combat overfitting and help our model generalize. Examples of regularization methods include:\n",
    "        <ul>\n",
    "            <li>Dropout.</li>\n",
    "            <li>L1 and L2 weight decay.</li>\n",
    "            <li>Data Augmentation.</li>\n",
    "            <li>Synthetic Data.</li>\n",
    "        </ul>\n",
    "    However, there is another regularization technique, it is <k><i>Label Smoothing</i></k>: Turns “hard” class label assignments to “soft” label assignments. Operates directly on the labels themselves. Is dead simple to implement. Can lead to a model that generalizes better.\n",
    "    </p>\n",
    "    <h2>Why would Label <i>Smoothing</i> be applied?</h2>\n",
    "    <p>\n",
    "       In image classification tasks, typically labels are thought as hard, binary assignments.\n",
    "    For example, by considering the following image from the MNIST dataset:\n",
    "    <br/>\n",
    "    <img src='images/3.jpg' style='width: 15%; height: 15%;'>\n",
    "    <br/>\n",
    "    The digit from the image above, is clearly a “3”, and in case of being necessary the one-hot encoded label vector for this data point it would look like the following:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    Notice how we’re performing hard label assignment here: all entries in the vector are 0 except for the 4th index (which corresponds to the digit 3) which is a 1.\n",
    "    <br/>\n",
    "    Hard label assignment is natural to us and maps to how our brains want to efficiently categorize and store information in neatly labeled and packaged boxes.\n",
    "    <br/>\n",
    "    If we were to apply soft label assignment to our one-hot encoded vector above it would now look like this:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code>[0.01 0.01 0.01 0.91 0.01 0.01 0.01 0.01 0.01 0.01]</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Notice how summing the list of values equals <code>1</code>, just like in the original one-hot encoded vector.\n",
    "    This type of label assignment is called <b>soft label assignment</b>.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Unlike hard label assignments where class labels are binary (i.e., positive for one class and a negative example\n",
    "    for all other classes), soft label assignment allows:\n",
    "    <ul>\n",
    "        <li>The positive class to have the largest probability.</li>\n",
    "        <li>While all other classes have a very small probability.</li>\n",
    "    </ul>\n",
    "    <h3>Benefits from Label <i>Smoothing</i></h3>\n",
    "    <br/>\n",
    "    The answer is that we don’t want our model to become too confident in its predictions. By applying label smoothing we can lessen the confidence of the model and prevent it from descending into deep crevices of the loss landscape where overfitting occurs.\n",
    "    </p>\n",
    "    <hr>\n",
    "    <h2>Time to Code</h2>\n",
    "    <br/>\n",
    "    First import the necessary packages, classes, libraries, etc..\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from learning_rate_schedulers import PolynomialDecay\n",
    "from minigooglenet import MiniGoogLeNet\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label smoothing by explicitly updating the labels list</h3>\n",
    "<br/>\n",
    "<p>\n",
    "    The <b>Label <i>Smoothing</i> by explicitly updating the labels list</b> implementation works by directly modification of the labels after one-hot encoding. All that it must be needed to do is implement a simple custom function.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    The method <code>smooth_labels(labels, factor=0.1)</code> is the core of the method by explicitly updating the labels list. The parameters of this method are:\n",
    "    <ul>\n",
    "        <li><code>labels</code>: Contains one-hot encoded labels for all data points in our dataset.</li>\n",
    "        <li><code>factor</code>: The optional “smoothing factor” is set to 10% by default.</li>\n",
    "    </ul>\n",
    "    <br/>\n",
    "    To start, let’s assume that the following one-hot encoded vector is supplied to our function:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    Notice how there is a hard label assignment, the true class labels is a <code>1</code> while all others are <code>0</code>.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Reduces the hard assignment label of <code>1</code> by the supplied <code>factor</code> amount. With <code>factor=0.1</code>, the operation <code>labels *= (1 - factor)</code> yields the following vector:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code>[0.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    Notice how the hard assignment of <code>1.0</code> has been dropped to <code>0.9</code>.\n",
    "    <br/>\n",
    "    The next step is to apply a very small amount of confidence to the rest of the class labels in the vector.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    A small amount of confidence can be done by taking <code>factor</code> and dividing it by the total number of possible class labels. In this case, there are 10 possible class labels so when <code>factor=0.1</code>, therefore, have<code>0.1 / 10 = 0.01</code> — that value is added to the vector on <code>labels += (factor / labels.shape[1])</code>, resulting in:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code>[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.91 0.01 0.01]</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    Notice how the “incorrect” classes here have a very small amount of confidence. It doesn’t seem like much, but in practice, it can help our model from overfitting.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Note: The <code>smooth_labels</code> function in part comes from <a href=\"https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/\">Chengwei’s article</a>, by discussing the <b>Bag of Tricks for Image Classification with Convolutional Neural Networks paper</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, factor=0.1):\n",
    "    # smooth the labels\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    " \n",
    "    # returned the smoothed labels\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_smoothing_value():\n",
    "    print('Please input the Smoothing value:')\n",
    "    SMOOTHING = input()\n",
    "    print('Smoothing value of {}'.format(SMOOTHING))\n",
    "    return SMOOTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Ask for the <code>SMOOTHING</code> factor.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTHING = input_smoothing_value()\n",
    "SMOOTHING = float(SMOOTHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Initialize three training hyperparameters including the total number of epochs to train for, initial learning rate, and batch size.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the total number of epochs to train for, initial learning\n",
    "# rate, and batch size\n",
    "NUM_EPOCHS = 32\n",
    "INIT_LR = 5e-3\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Initialize the class <code>labelNames</code>  for the <a href='https://en.wikipedia.org/wiki/CIFAR-10'>CIFAR-10</a> dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Handle loading <a href='https://en.wikipedia.org/wiki/CIFAR-10'>CIFAR-10</a> dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and testing data, converting the images from\n",
    "# integers to floats\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\")\n",
    "testX = testX.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Mean subtraction, a form of normalization for faster generelization.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mean subtraction to the data\n",
    "mean = np.mean(trainX, axis=0)\n",
    "trainX -= mean\n",
    "testX -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    One-hot encode the labels and convert them to floats.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors, converting the data\n",
    "# type to floats so we can apply label smoothing\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "trainY = trainY.astype(\"float\")\n",
    "testY = testY.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Applies label smoothing using the <code>smooth_labels</code> function.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply label smoothing to the *training labels only*\n",
    "print(\"[INFO] smoothing amount: {}\".format(SMOOTHING))\n",
    "print(\"[INFO] before smoothing: {}\".format(trainY[0]))\n",
    "trainY = smooth_labels(trainY, SMOOTHING)\n",
    "print(\"[INFO] after smoothing: {}\".format(trainY[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "     Instantiate the data augmentation object.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Initialize learning rate decay via a callback that will be executed at the start of each epoch. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the learning rate scheduler callback\n",
    "schedule = PolynomialDecay(maxEpochs=NUM_EPOCHS,\n",
    "                           initAlpha=INIT_LR,\n",
    "                           power=1.0)\n",
    "callbacks = [LearningRateScheduler(schedule)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Compile and Train the MiniGoogleNet model\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.7)\n",
    "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Save the model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('minigooglenet_explicit_smooth_labels.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Once the model is fully trained and saved, generate a classification report as well as a training history plot.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.89      0.91      0.90      1000\n",
      "  automobile       0.95      0.96      0.96      1000\n",
      "        bird       0.83      0.83      0.83      1000\n",
      "         cat       0.81      0.77      0.79      1000\n",
      "        deer       0.89      0.88      0.89      1000\n",
      "         dog       0.88      0.81      0.84      1000\n",
      "        frog       0.87      0.94      0.91      1000\n",
      "       horse       0.92      0.93      0.92      1000\n",
      "        ship       0.93      0.95      0.94      1000\n",
      "       truck       0.94      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "def plot_history_metrics(metric, val_metric, lbl_metric, lbl_val_metric, title, ylabel, plt_file_name):\n",
    "    N = np.arange(0, NUM_EPOCHS)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(N, metric, label=lbl_metric)\n",
    "    plt.plot(N, val_metric, label=lbl_val_metric)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(plt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(H.history[\"loss\"],\n",
    "                     H.history[\"val_loss\"],\n",
    "                     \"train_loss\",\n",
    "                     \"val_loss\",\n",
    "                     \"Training Loss vs Validation Loss\",\n",
    "                     \"Loss\",\n",
    "                     \"loss_value_label_smoothing_explicitly_updating_labels_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='loss_value_label_smoothing_explicitly_updating_labels_list.png' style='display: block; margin-left: auto; margin-right: auto; width: 50%;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(H.history[\"accuracy\"],\n",
    "                     H.history[\"val_accuracy\"],\n",
    "                     \"train_accuracy\",\n",
    "                     \"val_accuracy\",\n",
    "                     \"Training Accuracy vs Validation Accuracy\",\n",
    "                     \"Accuracy\",\n",
    "                     \"Accuracy_value_label_smoothing_explicitly_updating_labels_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Accuracy_value_label_smoothing_explicitly_updating_labels_list.png' style='display: block; margin-left: auto; margin-right: auto; width: 50%;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <h3>Label smoothing by using the loss function.</h3>\n",
    "    <br/>\n",
    "    <p>\n",
    "    The second method to implement <b>Label <i>Smoothing</i></b> utilizes <b>Keras/TensorFlow’s</b> <code>CategoricalCrossentropy</code> class directly.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    The benefit here is that there is not need to implement any custom function. <b>Label <i>Smoothing</i> can be applied on the fly when instantiating the</b> <code>CategoricalCrossentropy</code> <b>class with the </b> <code>label_smoothing parameter</code>:\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <div style='text-align: center;'>\n",
    "        <code> CategoricalCrossentropy(label_smoothing=0.1)</code>\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    The benefit here is that there is not need of any custom implementation, <i><b>but the downside is that we don’t have access to the raw labels list which would be a problem if you need it to compute your own custom metrics when monitoring the training process.</i></b>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    With all that said, let’s learn how to utilize the CategoricalCrossentropy for label smoothing. The process is really similar, there are only a few changes, which will be explained.\n",
    "    </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    As usual initialize the optimizer and loss. The core of the method by using the loss function is here in the loss method with <b>Label <i>Smoothing</i></b>: Notice how we’re passing in the <code>label_smoothing</code> parameter to the <code>CategoricalCrossentropy</code> class. This class will automatically apply label smoothing for us.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] smoothing amount: 0.1\n"
     ]
    }
   ],
   "source": [
    "# initialize the Optimizer and Loss\n",
    "print(\"[INFO] smoothing amount: {}\".format(SMOOTHING))\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9)\n",
    "loss = CategoricalCrossentropy(label_smoothing=SMOOTHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Then compile the model, passing in our loss with label smoothing. To wrap up, we’ll train our model, evaluate it, and plot the training history\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "WARNING:tensorflow:From <ipython-input-34-22b936fd0dbc>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 6250 steps, validate on 10000 samples\n",
      "Epoch 1/32\n",
      "6250/6250 [==============================] - 72s 12ms/step - loss: 0.8693 - accuracy: 0.8687 - val_loss: 0.5263 - val_accuracy: 0.8288\n",
      "Epoch 2/32\n",
      "6250/6250 [==============================] - 74s 12ms/step - loss: 0.8419 - accuracy: 0.8808 - val_loss: 0.4445 - val_accuracy: 0.8681\n",
      "Epoch 3/32\n",
      "6250/6250 [==============================] - 75s 12ms/step - loss: 0.8221 - accuracy: 0.8884 - val_loss: 0.4927 - val_accuracy: 0.8447\n",
      "Epoch 4/32\n",
      "6250/6250 [==============================] - 74s 12ms/step - loss: 0.8015 - accuracy: 0.8995 - val_loss: 0.4323 - val_accuracy: 0.8722\n",
      "Epoch 5/32\n",
      "6250/6250 [==============================] - 75s 12ms/step - loss: 0.7874 - accuracy: 0.9054 - val_loss: 0.4206 - val_accuracy: 0.8731\n",
      "Epoch 6/32\n",
      "6250/6250 [==============================] - 75s 12ms/step - loss: 0.7723 - accuracy: 0.9136 - val_loss: 0.4290 - val_accuracy: 0.8727\n",
      "Epoch 7/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.7586 - accuracy: 0.9203 - val_loss: 0.3926 - val_accuracy: 0.8827\n",
      "Epoch 8/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.7451 - accuracy: 0.9248 - val_loss: 0.4251 - val_accuracy: 0.8746\n",
      "Epoch 9/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.7352 - accuracy: 0.9302 - val_loss: 0.4190 - val_accuracy: 0.8734\n",
      "Epoch 10/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.7262 - accuracy: 0.9343 - val_loss: 0.4175 - val_accuracy: 0.8741\n",
      "Epoch 11/32\n",
      "6250/6250 [==============================] - 79s 13ms/step - loss: 0.7160 - accuracy: 0.9383 - val_loss: 0.4251 - val_accuracy: 0.8680\n",
      "Epoch 12/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.7069 - accuracy: 0.9427 - val_loss: 0.3551 - val_accuracy: 0.8976\n",
      "Epoch 13/32\n",
      "6250/6250 [==============================] - 78s 12ms/step - loss: 0.6975 - accuracy: 0.9473 - val_loss: 0.3738 - val_accuracy: 0.8872\n",
      "Epoch 14/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6855 - accuracy: 0.9526 - val_loss: 0.3724 - val_accuracy: 0.8912\n",
      "Epoch 15/32\n",
      "6250/6250 [==============================] - 78s 13ms/step - loss: 0.6812 - accuracy: 0.9546 - val_loss: 0.3578 - val_accuracy: 0.8985\n",
      "Epoch 16/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6723 - accuracy: 0.9586 - val_loss: 0.3860 - val_accuracy: 0.8834\n",
      "Epoch 17/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6652 - accuracy: 0.9617 - val_loss: 0.3494 - val_accuracy: 0.8974\n",
      "Epoch 18/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6602 - accuracy: 0.9633 - val_loss: 0.3579 - val_accuracy: 0.8949\n",
      "Epoch 19/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6533 - accuracy: 0.9666 - val_loss: 0.3536 - val_accuracy: 0.8969\n",
      "Epoch 20/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6476 - accuracy: 0.9694 - val_loss: 0.3713 - val_accuracy: 0.8903\n",
      "Epoch 21/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6408 - accuracy: 0.9726 - val_loss: 0.3464 - val_accuracy: 0.8990\n",
      "Epoch 22/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6346 - accuracy: 0.9750 - val_loss: 0.3560 - val_accuracy: 0.8985\n",
      "Epoch 23/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6323 - accuracy: 0.9760 - val_loss: 0.3486 - val_accuracy: 0.8972\n",
      "Epoch 24/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6276 - accuracy: 0.9779 - val_loss: 0.3603 - val_accuracy: 0.8991\n",
      "Epoch 25/32\n",
      "6250/6250 [==============================] - 79s 13ms/step - loss: 0.6238 - accuracy: 0.9796 - val_loss: 0.3579 - val_accuracy: 0.8981\n",
      "Epoch 26/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6194 - accuracy: 0.9813 - val_loss: 0.3452 - val_accuracy: 0.9012\n",
      "Epoch 27/32\n",
      "6250/6250 [==============================] - 78s 13ms/step - loss: 0.6161 - accuracy: 0.9825 - val_loss: 0.3464 - val_accuracy: 0.9018\n",
      "Epoch 28/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6127 - accuracy: 0.9845 - val_loss: 0.3450 - val_accuracy: 0.9009\n",
      "Epoch 29/32\n",
      "6250/6250 [==============================] - 79s 13ms/step - loss: 0.6113 - accuracy: 0.9847 - val_loss: 0.3412 - val_accuracy: 0.9028\n",
      "Epoch 30/32\n",
      "6250/6250 [==============================] - 78s 13ms/step - loss: 0.6081 - accuracy: 0.9862 - val_loss: 0.3402 - val_accuracy: 0.9019\n",
      "Epoch 31/32\n",
      "6250/6250 [==============================] - 81s 13ms/step - loss: 0.6071 - accuracy: 0.9859 - val_loss: 0.3392 - val_accuracy: 0.9023\n",
      "Epoch 32/32\n",
      "6250/6250 [==============================] - 77s 12ms/step - loss: 0.6042 - accuracy: 0.9883 - val_loss: 0.3386 - val_accuracy: 0.9022\n"
     ]
    }
   ],
   "source": [
    "# Train the MiniGoogleNet network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in case of being necessary\n",
    "model.save(\"minigooglenet_by_loss_function.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.91      0.91      0.91      1000\n",
      "  automobile       0.96      0.96      0.96      1000\n",
      "        bird       0.85      0.86      0.85      1000\n",
      "         cat       0.82      0.80      0.81      1000\n",
      "        deer       0.90      0.90      0.90      1000\n",
      "         dog       0.88      0.85      0.86      1000\n",
      "        frog       0.90      0.94      0.92      1000\n",
      "       horse       0.94      0.92      0.93      1000\n",
      "        ship       0.94      0.94      0.94      1000\n",
      "       truck       0.94      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate The Network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(H.history[\"accuracy\"],\n",
    "                     H.history[\"val_accuracy\"],\n",
    "                     \"train_accuracy\",\n",
    "                     \"val_accuracy\",\n",
    "                     \"Training Accuracy vs Validation Accuracy\",\n",
    "                     \"Accuracy\",\n",
    "                     \"accuracy_value_label_smoothing_by_loss_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='accuracy_value_label_smoothing_by_loss_function.png' style='display: block; margin-left: auto; margin-right: auto; width: 50%;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(H.history[\"loss\"],\n",
    "                     H.history[\"val_loss\"],\n",
    "                     \"train_loss\",\n",
    "                     \"val_loss\",\n",
    "                     \"Training Loss vs Validation Loss\",\n",
    "                     \"Loss\",\n",
    "                     \"loss_value_label_smoothing_by_loss_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='loss_value_label_smoothing_by_loss_function.png' style='display: block; margin-left: auto; margin-right: auto; width: 50%;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    A score of ~90% accuracy, but that does not mean that the <code>CategoricalCrossentropy</code> method is “better” than the <code>smooth_labels</code> technique. For all intents and purposes these results are “equal” and would show to follow the same distribution if the results were averaged over multiple runs.\n",
    "    <br/>\n",
    "    <h2>A Strange Behaviour</h2> <h3>Validation Loss is way more lower than Training Loss <i>but Training Accuracy is way more higher than Validation Accuracy</i></h3>\n",
    "    <br/>\n",
    "    Note that the validation loss is lower than our training loss yet our training accuracy is higher than our validation accuracy, <i><b>this is totally normal behavior when using label smoothing so don’t be alarmed by it.</i></b>\n",
    "    <br/>\n",
    "    <h3>Remember when to apply Label <i>Smoothing</i></h3>\n",
    "    <br/>\n",
    "    It is recommended to apply label smoothing when there is trouble getting the model to generalize and/or the model is overfitting to the training set. When those situations happen regularization techniques must be applied. Label smoothing is just one type of regularization, however. Other types of regularization include:\n",
    "    <ul>\n",
    "        <li>Dropout.</li>\n",
    "        <li>L1, L2, etc. weight decay.</li>\n",
    "        <li>Data augmentation.</li>\n",
    "        <li>Decreasing model capacity.</li>\n",
    "    </ul>\n",
    "    <hr>\n",
    "    <h2>Summary</h2>\n",
    "    <p>\n",
    "        This notebook brings a brief practical explanation of how to implement two methods to apply <i>Label Smoothing</i> using Keras, TensorFlow, and Deep Learning:\n",
    "    <ol>\n",
    "        <li>Label smoothing by updating your labels lists using a custom label parsing function</li>\n",
    "        <li>Label smoothing using the loss function in TensorFlow/Keras.</li>\n",
    "    </ol>\n",
    "    Label Smoothing can be seen as a form of regularization that improves the ability of the model to generalize to testing data, <b><i>but perhaps at the expense of accuracy on your training set, typically this tradeoff is well worth it.</b></i>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    It is normally recommend apply Label Smoothing by updating your labels lists using a custom label parsing function when either:\n",
    "    <ul>\n",
    "        <li>Entire dataset fits into memory and you can smooth all labels in a single function call.</li>\n",
    "        <li>Need direct access to the label variables.</li>\n",
    "    </ul>\n",
    "    \n",
    "Otherwise, Label smoothing using the loss function in TensorFlow/Keras tends to be easier to utilize as:\n",
    "    <ol>\n",
    "    <li>It’s baked right into Keras/TensorFlow</li>\n",
    "    <li>Does not require any hand-implemented functions.</li>\n",
    "\n",
    "Regardless of which method is chosen, they both do the same thing, <b><i>Smooth the labels</b></i>, thereby attempting to improve the ability of the model to generalize.\n",
    "    </p>\n",
    "    <hr>\n",
    "    <h3>References</h3>\n",
    "    <br/>\n",
    "    <ul>\n",
    "        <li><a href='https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/'>Bag of Tricks for Image Classification with Convolutional Neural Networks in Keras.</a></li>\n",
    "        <li><a href='https://leimao.github.io/blog/Label-Smoothing/'>Label Smoothing by Lei Mao.</a></li>\n",
    "        <li><a href='https://arxiv.org/abs/1906.02629'>When Does Label Smoothing Help?</a></li>\n",
    "        <li><a href='https://arxiv.org/abs/1812.01187'>Bag of Tricks for Image Classification with Convolutional Neural Networks.</a></li>\n",
    "        <li><a href='https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/'>Label Smoothing by Adrian Rosebrock.</a></li>\n",
    "    </ul>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
